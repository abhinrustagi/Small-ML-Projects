{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping using Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An open source and collaborative framework for extracting the data you need from websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation: ```pip install scrapy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency - [Microsoft Visual C++ Build Tools](http://go.microsoft.com/fwlink/?LinkId=691126&fixForIE=.exe)\n",
    "<br>\n",
    "If there is an error due to rc.exe, use this link : [StackOverflow](https://stackoverflow.com/questions/43858836/python-installing-clarifai-vs14-0-link-exe-failed-with-exit-status-1158)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up a scrapy project, go to the desired folder and open terminal and run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'myfirstproject', using template directory 'c:\\users\\rusta\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    C:\\Users\\rusta\\Documents\\Python Scripts\\Using Scrapy\\myfirstproject\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd myfirstproject\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject myfirstproject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spider is a class that allows to scrape information from a website. They have a base class scrapy.spider from which they inherit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write a spider class, go to the spiders folder in the directory and write the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    # Name of the Spider should be in single quotes\n",
    "    name = 'quotes'\n",
    "    \n",
    "    # To make GET request\n",
    "    def start_request(self):\n",
    "        \n",
    "        #Write URL in single quotes as well\n",
    "        urls = ['http://quotes.toscrape.com/page/1/']\n",
    "        \n",
    "        # Generator Function\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback = self.parse)\n",
    "    \n",
    "    def parse(self, response):\n",
    "        page_id = response.url.split(\"/\")[-2]\n",
    "        \n",
    "        filename = \"quotes-%s.html\"%page_id\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log(\"Saved file %s\"%filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this, go to directory, open Terminal and type the command: ```scrapy crawl quotes```. It will svae the page as HTML file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Shell to extract information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape a page in shell : ```scrapy shell \"url\"```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check response: ```response```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get into data using CSS selectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```response.css('title')``` : Returns a list of all such elements, also contains metadata.\n",
    "<br>\n",
    "```response.css('title').getall()```: Returns the HTML part\n",
    "<br>\n",
    "```response.css('title::text').getall()``` : To get a list of the text\n",
    "<br>\n",
    "```response.css('title::text').get()``` : Returns first element of the list\n",
    "<br>\n",
    "```response.css(\"div.quote\").getall()``` : div is the tag, quote is the class\n",
    "<br>\n",
    "```quote = response.css('div.quote')[0]``` <br>\n",
    "```title0 = quote.css(\"span.text\").get()``` <br>\n",
    "```title0 = quote.css(\"span.text::text\").get()``` <br>\n",
    "```author = quote.css('small.author::text').get()``` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    # Name of the Spider should be in single quotes\n",
    "    name = 'quotes'\n",
    "    \n",
    "    # To make GET request\n",
    "    def start_request(self):\n",
    "        \n",
    "        #Write URL in single quotes as well\n",
    "        urls = ['http://quotes.toscrape.com/page/1/']\n",
    "        \n",
    "        # Generator Function\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback = self.parse)\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \n",
    "        for q in response.css(\"div.quote\"):\n",
    "            text = q.css(\"span.text::text\").get()\n",
    "            author = q.css(\"small.author::text\").get()\n",
    "            tags = q.css(\"a.tag::text\").getall()\n",
    "            \n",
    "            yield {\n",
    "                'text' : text,\n",
    "                'author' : author,\n",
    "                'tags' : tags\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call above spider : ```scrapy crawl quotes -o quotes.json``` \n",
    "<br> ```-o``` is a command line command to write files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for next button: ```response.css('li.next a').get()```\n",
    "<br>Extracting ```href```: ```response.css('li.next a::attr(href)').get()```\n",
    "<br> Another way: ```response.css('li.next a:').attrib[\"href\"]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    # Name of the Spider should be in single quotes\n",
    "    name = 'quotes'\n",
    "    \n",
    "    # To make GET request\n",
    "    def start_request(self):\n",
    "        \n",
    "        #Write URL in single quotes as well\n",
    "        urls = ['http://quotes.toscrape.com/page/1/']\n",
    "        \n",
    "        # Generator Function\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback = self.parse)\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \n",
    "        for q in response.css(\"div.quote\"):\n",
    "            text = q.css(\"span.text::text\").get()\n",
    "            author = q.css(\"small.author::text\").get()\n",
    "            tags = q.css(\"a.tag::text\").getall()\n",
    "            \n",
    "            yield {\n",
    "                'text' : text,\n",
    "                'author' : author,\n",
    "                'tags' : tags\n",
    "            }\n",
    "        \n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback = self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
